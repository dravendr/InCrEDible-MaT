{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc41c1d",
   "metadata": {},
   "source": [
    "# Train ANN Model Alkaline HER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9aaee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import sklearn\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import ensemble\n",
    "from sklearn.tree import ExtraTreeRegressor\n",
    "from sklearn import svm\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import pandas as pd\n",
    "#%matplotlib \n",
    "###########import packages##########\n",
    "import catboost\n",
    "import xgboost\n",
    "import lightgbm\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import *\n",
    "import pickle\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import  *\n",
    "###########import packages##########\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm \n",
    "# from keras.wrappers import scikit_learn\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "###########loading data##########\n",
    "loo = LeaveOneOut()\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf4a2ba",
   "metadata": {},
   "source": [
    "# ANN needs scalers to regularize input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a5a7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "Minmaxsc  = MinMaxScaler(feature_range=(0, 1))\n",
    "Minmaxsc2  = MinMaxScaler(feature_range=(0, 1))\n",
    "Stdsc  = StandardScaler()\n",
    "Stdsc2  = StandardScaler()\n",
    "MAsc  = MaxAbsScaler()\n",
    "MAsc2  = MaxAbsScaler()\n",
    "Rsc  = RobustScaler()\n",
    "Rsc2  = RobustScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb8163",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########wrapping root mean square error for later calls##########\n",
    "def compute_mae_mse_rmse_r2_r_rs_tau(target,prediction):\n",
    "    target = target.flatten()  # reshape to 1D array\n",
    "    prediction = prediction.flatten()  # reshape to 1D array\n",
    "    error = target - prediction\n",
    "    squaredError = error ** 2\n",
    "    absError = np.abs(error)\n",
    "    mae = np.mean(absError)  # 平均绝对误差MAE\n",
    "    mse = np.mean(squaredError)  # 均方误差MSE\n",
    "    rmse = np.sqrt(mse)  # 均方根误差RMSE\n",
    "    r2 = r2_score(prediction,target)  # R2 score\n",
    "    r, _ = pearsonr(target, prediction)  # Pearson's correlation coefficient (R)\n",
    "    rs, _ = spearmanr(target, prediction)  # Spearman's rank correlation coefficient (Rs)\n",
    "    tau, _ = kendalltau(target, prediction)  # Kendall's rank correlation coefficient (Tau)\n",
    "    return mae, mse, rmse, r2, r, rs, tau\n",
    "def gridsearch(model,param,algorithm_name,X,y):\n",
    "    grid = GridSearchCV(model,param_grid=param,scoring='neg_root_mean_squared_error',cv=5,n_jobs=-1,verbose=2)\n",
    "    grid.fit(X,y)\n",
    "    best_model=grid.best_estimator_\n",
    "    best_score=grid.best_score_\n",
    "    print('Best Regressor:',grid.best_params_,'Best Score:', best_score)\n",
    "    return best_model,best_score\n",
    "seed=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc8892c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def create_dropout_predict_function(model, dropout):\n",
    "    \"\"\"\n",
    "    Create a predict function that enables dropout during prediction time.\n",
    "    \"\"\"\n",
    "    # Get the layers and their rates\n",
    "    layers = [l for l in model.layers if hasattr(l, 'rate')]\n",
    "\n",
    "    # Define a function to apply dropout\n",
    "    def apply_dropout():\n",
    "        for layer in layers:\n",
    "            if hasattr(layer, 'rate'):\n",
    "                layer.rate = dropout\n",
    "\n",
    "    # Define a function to predict with dropout\n",
    "    def predict_with_dropout(x):\n",
    "        apply_dropout()\n",
    "        return model(x)\n",
    "\n",
    "    return K.function([model.input], [predict_with_dropout(model.input)])\n",
    "import time\n",
    "\n",
    "def predict_with_dropout(model, x, n_iterations=1000000):\n",
    "    predictions = []\n",
    "    start_time = time.time()  # Start timer\n",
    "    for i in range(n_iterations):\n",
    "        predictions.append(dropout_predict([x]))\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    mean_predictions = predictions.mean(axis=0)\n",
    "    std_predictions = predictions.std(axis=0)\n",
    "    end_time = time.time()  # End timer\n",
    "    elapsed_time = end_time - start_time  # Calculate elapsed time\n",
    "\n",
    "    print(f\"Time taken: {elapsed_time} seconds\")\n",
    "    return mean_predictions, std_predictions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e27b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_print_with_uncertainty(algorithm_name, model, random_seed):\n",
    "    X_train,X_test,y_train,y_test=train_test_split(data_input_full_ANN,data_output_full_ANN,test_size=0.05,random_state=random_seed)\n",
    "    \n",
    "    # Make predictions with dropout\n",
    "    mean_pred_train, std_pred_train = predict_with_dropout(model, X_train)\n",
    "    mean_pred_test, std_pred_test = predict_with_dropout(model, X_test)\n",
    "    mean_pred_all, std_pred_all = predict_with_dropout(model, data_input_full_ANN)\n",
    "\n",
    "    # Convert predictions back to original scale\n",
    "    mean_pred_train = Stdsc2.inverse_transform(mean_pred_train.mean(axis=1).reshape(-1, 1))\n",
    "    mean_pred_test = Stdsc2.inverse_transform(mean_pred_test.mean(axis=1).reshape(-1, 1))\n",
    "    mean_pred_all = Stdsc2.inverse_transform(mean_pred_all.mean(axis=1).reshape(-1, 1))\n",
    "    real_train=Stdsc2.inverse_transform(y_train)\n",
    "    real_test=Stdsc2.inverse_transform(y_test)\n",
    "    real_all=Stdsc2.inverse_transform(data_output_full_ANN)\n",
    "    \n",
    "    # Print metrics\n",
    "    print(compute_mae_mse_rmse_r2_r_rs_tau(mean_pred_all,real_all))\n",
    "    print(compute_mae_mse_rmse_r2_r_rs_tau(mean_pred_test,real_test))\n",
    "    print(compute_mae_mse_rmse_r2_r_rs_tau(mean_pred_train,real_train))\n",
    "    \n",
    "    # Convert standard deviations back to original scale\n",
    "    std_pred_train_orig = std_pred_train * Stdsc2.scale_\n",
    "    std_pred_test_orig = std_pred_test * Stdsc2.scale_\n",
    "    \n",
    "        # Plot predictions\n",
    "    fig=plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    x_y_x=np.arange(0,700,1)\n",
    "    x_y_y=np.arange(0,700,1)\n",
    "    ax.scatter(mean_pred_train,real_train,c='blue',label='Train',alpha=0.25)\n",
    "    ax.scatter(mean_pred_test,real_test,c='red',label='Test',alpha=0.75)\n",
    "    ax.plot(x_y_x,x_y_y,c='black')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Predicted_Overpotential (mV)')\n",
    "    plt.ylabel('Real_Overpotential (mV)')\n",
    "    plt.savefig(algorithm_name+'_Alkaline_HER.png')\n",
    "\n",
    "    # Create a new figure with two subplots side by side\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "    # Create an array of indices for bar plot\n",
    "    indices_test = np.arange(len(mean_pred_test))\n",
    "    indices_train = np.arange(len(mean_pred_train))\n",
    "\n",
    "    # Plot standard deviations for test data\n",
    "    axs[0, 0].bar(indices_test, std_pred_test_orig.flatten(), color='red', alpha=0.75, label='Test')\n",
    "\n",
    "    # Plot standard deviations for train data\n",
    "    axs[0, 1].bar(indices_train, std_pred_train_orig.flatten(), color='blue', alpha=0.75, label='Train')\n",
    "\n",
    "    # Add horizontal line for average standard deviation\n",
    "    avg_std_test = np.mean(std_pred_test_orig)\n",
    "    avg_std_train = np.mean(std_pred_train_orig)\n",
    "    axs[0, 0].axhline(avg_std_test, color='purple', linestyle='--', label=f'Avg Std (Test): {avg_std_test:.2e}')\n",
    "    axs[0, 1].axhline(avg_std_train, color='purple', linestyle='--', label=f'Avg Std (Train): {avg_std_train:.2e}')\n",
    "\n",
    "    # Set labels and legend\n",
    "    for ax in axs[0]:\n",
    "        ax.legend()\n",
    "        ax.set_xlabel('Sample Index')\n",
    "        ax.set_ylabel('Standard Deviation of Predictions')\n",
    "\n",
    "    # Set y-limits to be the same for both plots\n",
    "    ylim = max(axs[0, 0].get_ylim()[1], axs[0, 1].get_ylim()[1])\n",
    "    for ax in axs[0]:\n",
    "        ax.set_ylim(0, ylim)\n",
    "\n",
    "    # Plot histograms of standard deviations\n",
    "    axs[1, 0].hist(std_pred_test_orig.flatten(), bins=50, color='red', alpha=0.75)\n",
    "    axs[1, 0].set_title('Histogram of Standard Deviations (Test)')\n",
    "    axs[1, 0].set_xlabel('Standard Deviation')\n",
    "    axs[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "    axs[1, 1].hist(std_pred_train_orig.flatten(), bins=50, color='blue', alpha=0.75)\n",
    "    axs[1, 1].set_title('Histogram of Standard Deviations (Train)')\n",
    "    axs[1, 1].set_xlabel('Standard Deviation')\n",
    "    axs[1, 1].set_ylabel('Frequency')\n",
    "    \n",
    "    # Rotate xticks by 45 degrees\n",
    "    for ax in axs.flat:\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45)\n",
    "    \n",
    "    # Save figure\n",
    "    plt.savefig(algorithm_name+'_Alkaline_HER_std_combined.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5159ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ANN_model_3layer(X,learning_rate,regular_term=0.001,neuron_number=50,drop_out_rate=0.1):\n",
    "    regularizer=keras.regularizers.l2(regular_term)\n",
    "    model = Sequential() \n",
    "    model.add(Dense(neuron_number, input_dim=X.shape[1], kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))    \n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(1, input_dim=neuron_number, activation='linear'))\n",
    "    adam=optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0516abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ANN_model_3layer_CV(X,learning_rate,regular_term=0.001,neuron_number=50,drop_out_rate=0.1):\n",
    "    regularizer=keras.regularizers.l2(regular_term)\n",
    "    model = Sequential() \n",
    "    model.add(Dense(neuron_number, input_dim=X.shape[1], kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))    \n",
    "    model.add(Dense(neuron_number, input_dim=neuron_number, kernel_initializer='random_normal',\n",
    "                    bias_initializer='random_normal',activation='relu',kernel_regularizer=regularizer)) \n",
    "    model.add(Dropout(drop_out_rate))\n",
    "    model.add(Dense(1, input_dim=neuron_number, activation='linear'))\n",
    "    adam=optimizers.Adam(learning_rate)\n",
    "    model.compile(loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aacf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "database=pd.read_csv('processed_database.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output_full=database.iloc[:,1]\n",
    "data_input_full=database.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf70741",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input_full_ANN=Stdsc.fit_transform(data_input_full)\n",
    "data_output_full_ANN=Stdsc2.fit_transform(np.array(data_output_full).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1a3c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "model_ANNRegressor_CV= KerasRegressor(build_fn=create_ANN_model_3layer_CV(X=data_input_full,learning_rate=0.001), \n",
    "                                   epochs=50, batch_size=32, verbose=0)\n",
    "def predict_and_print_cv(model, n_splits=5):\n",
    "    kfold = KFold(n_splits=n_splits, shuffle=True, random_state=7)\n",
    "    results_all, results_test, results_train = [], [], []\n",
    "\n",
    "    for train_index, test_index in kfold.split(data_input_full_ANN):\n",
    "        X_train, X_test = data_input_full_ANN[train_index], data_input_full_ANN[test_index]\n",
    "        y_train, y_test = data_output_full_ANN[train_index], data_output_full_ANN[test_index]\n",
    "\n",
    "        model.fit(X_train, y_train)  # assuming your model needs to be retrained in each fold\n",
    "\n",
    "        pred_train = Stdsc2.inverse_transform(model.predict(X_train))\n",
    "        pred_test = Stdsc2.inverse_transform(model.predict(X_test))\n",
    "        real_train = Stdsc2.inverse_transform(y_train)\n",
    "        real_test = Stdsc2.inverse_transform(y_test)\n",
    "\n",
    "        pred_all = Stdsc2.inverse_transform(model.predict(data_input_full_ANN))\n",
    "        real_all = Stdsc2.inverse_transform(data_output_full_ANN)\n",
    "\n",
    "        results_all.append(compute_mae_mse_rmse_r2_r_rs_tau(pred_all, real_all))\n",
    "        results_test.append(compute_mae_mse_rmse_r2_r_rs_tau(pred_test, real_test))\n",
    "        results_train.append(compute_mae_mse_rmse_r2_r_rs_tau(pred_train, real_train))\n",
    "\n",
    "    # Compute average metrics over all folds\n",
    "    avg_results_all = np.mean(results_all, axis=0)\n",
    "    avg_results_test = np.mean(results_test, axis=0)\n",
    "    avg_results_train = np.mean(results_train, axis=0)\n",
    "\n",
    "    print(\"Average metrics over all data: \", avg_results_all)\n",
    "    print(\"Average metrics over test data: \", avg_results_test)\n",
    "    print(\"Average metrics over train data: \", avg_results_train)\n",
    "\n",
    "    return avg_results_all, avg_results_test, avg_results_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63ed15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ANNRegressor= KerasRegressor(build_fn=create_ANN_model_3layer(X=data_input_full,learning_rate=0.001), verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a6b234",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_list = [32,64]\n",
    "optimizers_list=['adam','sgd']\n",
    "epochs_list=[50,100,150,200]\n",
    "param_ann = dict(batch_size=batch_size_list, \n",
    "                 epochs=epochs_list,\n",
    "                optimizer=optimizers_list\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d26c3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "tf.compat.v1.set_random_seed(seed)\n",
    "X_train_ANN,X_test_ANN,y_train_ANN,y_test_ANN=train_test_split(data_input_full_ANN,data_output_full_ANN,test_size=0.05,random_state=seed)\n",
    "ANN_3layer,ANN_3layer_score=gridsearch(model_ANNRegressor,param_ann,'Artificial Neural Network_3layer',X_train_ANN,y_train_ANN)\n",
    "predict_and_print('Artificial Neural Network_3layer',ANN_3layer,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_3layer.model_.save('Alkaline_HER.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e16f271",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Call the function for getting CV average metrics\n",
    "avg_results_all, avg_results_test, avg_results_train = predict_and_print_cv(model_ANNRegressor_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANN_3layer=keras.models.load_model(\"Alkaline_HER.h5\")\n",
    "dropout_predict = create_dropout_predict_function(ANN_3layer, 0.25)\n",
    "predict_and_print_with_uncertainty('Artificial Neural Network_3layer',ANN_3layer,seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179fbb64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
