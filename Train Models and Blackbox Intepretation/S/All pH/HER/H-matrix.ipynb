{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4014a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextlib import contextmanager\n",
    "import os\n",
    "import sys\n",
    "from joblib import Parallel, delayed\n",
    "import itertools\n",
    "from pdpbox.pdp_calc_utils import _calc_ice_lines_inter\n",
    "from pdpbox.pdp import pdp_isolate, PDPInteract\n",
    "from pdpbox.utils import (_check_model, _check_dataset, _check_percentile_range, _check_feature,\n",
    "                    _check_grid_type, _check_memory_limit, _make_list,\n",
    "                    _calc_memory_usage, _get_grids, _get_grid_combos, _check_classes)\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "class ML_algorithm_committee:\n",
    "    def __init__(self, algorithms=None, **train_options):\n",
    "        \"\"\"\n",
    "        Initializes the ML_algorithm_committee with the provided algorithms.\n",
    "        If no algorithms are provided, use a default list of 14 algorithms.\n",
    "        \"\"\"\n",
    "        ####attributes get from .tran_models method###\n",
    "        self.models = None\n",
    "        self.models_scores = None\n",
    "        self.hyperparameters_optimization_results = None\n",
    "        self.scaler_X = None\n",
    "        self.scaler_y = None\n",
    "        ###current data, once models trained, able for users to check what current model committee used for training###\n",
    "        ###also, allow .importance_analysis to directly call the dataset without need to indicate X, y etc again###\n",
    "        self.current_data = None\n",
    "        self.problem_type = None\n",
    "        self.num_classes = None\n",
    "        ###attributes get from .importance_analysis###\n",
    "        ###analysis type: SHAP/PDP/permutation or weighted among them, default to be SHAP###\n",
    "        ###weighted/original;weighted method: r/r2 (r2 default) or accuraccy/f1-score/precision/recall/AUC for classification (AUC default)###\n",
    "        self.feature_importances = None\n",
    "        self.performance_ranking = None \n",
    "        self.best_models = None\n",
    "        self.feature_ranking = None\n",
    "        self.top_features = None\n",
    "        #######################\n",
    "        self.second_order_matrix=None\n",
    "        ###visulization part###\n",
    "        #######################\n",
    "        ###scatter plot for regression prediction###\n",
    "        ###AUC etc for classification prediction###\n",
    "        ###output importance/ranking heatmap plot (SHAP/PDP/permutation or weighted among them, default to be SHAP) (heatmap/vionlin plot) (weighted/original) (weighted method: r/r2 etc.) feature importance and ML algorithm###\n",
    "        ###default algorithms list###\n",
    "        self.default_algorithm_list = ['xgboost', 'catboost', 'lightgbm', 'svm', 'knn', 'gradient_boost',\n",
    "                                       'random_forest', 'decision_tree', 'extra_trees', 'adaboost',\n",
    "                                       'hist_gradient_boosting', 'deepforest', 'ann', 'cnn']\n",
    "\n",
    "    def pdp_multi_interact(self, model, dataset, model_features, features, \n",
    "                    num_grid_points=None, grid_types=None, percentile_ranges=None, grid_ranges=None, cust_grid_points=None, \n",
    "                    cust_grid_combos=None, use_custom_grid_combos=False,\n",
    "                    memory_limit=0.9, n_jobs=-1, predict_kwds=None, data_transformer=None):\n",
    "\n",
    "        if n_jobs==-1:\n",
    "            n_jobs=1\n",
    "\n",
    "        def _expand_default(x, default, length):\n",
    "            if x is None:\n",
    "                return [default] * length\n",
    "            return x\n",
    "\n",
    "        def _get_grid_combos(feature_grids, feature_types):\n",
    "            grids = [np.array(list(feature_grid),dtype=np.float16) for feature_grid in feature_grids]\n",
    "            for i in range(len(feature_types)):\n",
    "                if feature_types[i] == 'onehot':\n",
    "                    grids[i] = np.eye(len(grids[i])).astype(int).tolist()\n",
    "            return np.stack(np.meshgrid(*grids,copy=bool), -1).reshape(-1, len(grids))\n",
    "\n",
    "        if predict_kwds is None:\n",
    "            predict_kwds = dict()\n",
    "\n",
    "        nr_feats = len(features)\n",
    "\n",
    "        # check function inputs\n",
    "        n_classes, predict = _check_model(model=model)\n",
    "        _check_dataset(df=dataset)\n",
    "        _dataset = dataset.copy()\n",
    "\n",
    "        # prepare the grid\n",
    "        pdp_isolate_outs = []\n",
    "        if use_custom_grid_combos:\n",
    "            grid_combos = cust_grid_combos\n",
    "            feature_grids = []\n",
    "            feature_types = []\n",
    "        else:\n",
    "            num_grid_points = _expand_default(x=num_grid_points, default=5, length=nr_feats)\n",
    "            grid_types = _expand_default(x=grid_types, default='percentile', length=nr_feats)\n",
    "            for i in range(nr_feats):\n",
    "                _check_grid_type(grid_type=grid_types[i])\n",
    "\n",
    "            percentile_ranges = _expand_default(x=percentile_ranges, default=None, length=nr_feats)\n",
    "            for i in range(nr_feats):\n",
    "                _check_percentile_range(percentile_range=percentile_ranges[i])\n",
    "\n",
    "            grid_ranges = _expand_default(x=grid_ranges, default=None, length=nr_feats)\n",
    "            cust_grid_points = _expand_default(x=cust_grid_points, default=None, length=nr_feats)\n",
    "\n",
    "            _check_memory_limit(memory_limit=memory_limit)\n",
    "\n",
    "            pdp_isolate_outs = []\n",
    "            for idx in range(nr_feats):\n",
    "                pdp_isolate_out = pdp_isolate(\n",
    "                    model=model, dataset=_dataset, model_features=model_features, feature=features[idx],\n",
    "                    num_grid_points=num_grid_points[idx], grid_type=grid_types[idx], percentile_range=percentile_ranges[idx],\n",
    "                    grid_range=grid_ranges[idx], cust_grid_points=cust_grid_points[idx], memory_limit=memory_limit,\n",
    "                    n_jobs=n_jobs, predict_kwds=predict_kwds, data_transformer=data_transformer)\n",
    "                pdp_isolate_outs.append(pdp_isolate_out)\n",
    "\n",
    "            if n_classes > 2:\n",
    "                feature_grids = [pdp_isolate_outs[i][0].feature_grids for i in range(nr_feats)]\n",
    "                feature_types = [pdp_isolate_outs[i][0].feature_type  for i in range(nr_feats)]\n",
    "            else:\n",
    "                feature_grids = [pdp_isolate_outs[i].feature_grids for i in range(nr_feats)]\n",
    "                feature_types = [pdp_isolate_outs[i].feature_type  for i in range(nr_feats)]\n",
    "\n",
    "            grid_combos = _get_grid_combos(feature_grids, feature_types)\n",
    "\n",
    "        feature_list = []\n",
    "        for i in range(nr_feats):\n",
    "            feature_list.extend(_make_list(features[i]))\n",
    "\n",
    "        # Parallel calculate ICE lines\n",
    "        true_n_jobs = _calc_memory_usage(\n",
    "            df=_dataset, total_units=len(grid_combos), n_jobs=n_jobs, memory_limit=memory_limit)\n",
    "\n",
    "        grid_results = Parallel(n_jobs=true_n_jobs)(delayed(_calc_ice_lines_inter)(\n",
    "            grid_combo, data=_dataset, model=model, model_features=model_features, n_classes=n_classes,\n",
    "            feature_list=feature_list, predict_kwds=predict_kwds, data_transformer=data_transformer)\n",
    "                                                    for grid_combo in grid_combos)\n",
    "\n",
    "        ice_lines = pd.concat(grid_results, axis=0).reset_index(drop=True)\n",
    "        pdp = ice_lines.groupby(feature_list, as_index=False).mean()\n",
    "\n",
    "        # combine the final results\n",
    "        pdp_interact_params = {'n_classes': n_classes, \n",
    "                            'features': features, \n",
    "                            'feature_types': feature_types,\n",
    "                            'feature_grids': feature_grids}\n",
    "        if n_classes > 2:\n",
    "            pdp_interact_out = []\n",
    "            for n_class in range(n_classes):\n",
    "                _pdp = pdp[feature_list + ['class_%d_preds' % n_class]].rename(\n",
    "                    columns={'class_%d_preds' % n_class: 'preds'})\n",
    "                pdp_interact_out.append(\n",
    "                    PDPInteract(which_class=n_class,\n",
    "                                pdp_isolate_outs=[pdp_isolate_outs[i][n_class] for i in range(nr_feats)],\n",
    "                                pdp=_pdp, **pdp_interact_params))\n",
    "        else:\n",
    "            pdp_interact_out = PDPInteract(\n",
    "                which_class=None, pdp_isolate_outs=pdp_isolate_outs, pdp=pdp, **pdp_interact_params)\n",
    "\n",
    "        return pdp_interact_out\n",
    "\n",
    "    def center(self, arr): \n",
    "        return arr - np.mean(arr)\n",
    "\n",
    "    def compute_f_vals(self,mdl, X, features, selectedfeatures, num_grid_points=5, use_data_grid=False):\n",
    "        f_vals = {}\n",
    "        data_grid = None\n",
    "        if use_data_grid:\n",
    "            data_grid = X[selectedfeatures].values\n",
    "        \n",
    "        # check function inputs\n",
    "        n_classes, predict = _check_model(model=mdl)\n",
    "\n",
    "        # Calculate partial dependencies for full feature set\n",
    "        p_full = self.pdp_multi_interact(mdl, X, features, selectedfeatures, \n",
    "                                    num_grid_points=[num_grid_points] * len(selectedfeatures),\n",
    "                                    cust_grid_combos=data_grid,\n",
    "                                    use_custom_grid_combos=use_data_grid)\n",
    "        if n_classes >2:\n",
    "            p_full_pdp=p_full[0].pdp\n",
    "            for i in range(1,len(p_full)):\n",
    "                p_full_pdp+=p_full[i].pdp\n",
    "            p_full_pdp=p_full_pdp/n_classes\n",
    "            f_vals[tuple(selectedfeatures)] = self.center(p_full_pdp.preds.values)\n",
    "            grid = p_full_pdp.drop('preds', axis=1)\n",
    "        else:\n",
    "            f_vals[tuple(selectedfeatures)] = self.center(p_full.pdp.preds.values)\n",
    "            grid = p_full.pdp.drop('preds', axis=1)\n",
    "        # Calculate partial dependencies for [1..SFL-1]\n",
    "        for n in range(1, len(selectedfeatures)):\n",
    "            for subsetfeatures in itertools.combinations(selectedfeatures, n):\n",
    "                if use_data_grid:\n",
    "                    data_grid = X[list(subsetfeatures)].values\n",
    "                p_partial = self.pdp_multi_interact(mdl, X, features, subsetfeatures, \n",
    "                                            num_grid_points=[num_grid_points] * len(selectedfeatures),\n",
    "                                            cust_grid_combos=data_grid,\n",
    "                                            use_custom_grid_combos=use_data_grid)\n",
    "                \n",
    "                if n_classes >2:\n",
    "                    p_partial_pdp=p_partial[0].pdp\n",
    "                    for i in range(1,len(p_partial)):\n",
    "                        p_partial_pdp+=p_partial[i].pdp\n",
    "                    p_partial_pdp=p_partial_pdp/n_classes\n",
    "                    p_joined = pd.merge(grid, p_partial_pdp, how='left')\n",
    "                else:\n",
    "                    p_joined = pd.merge(grid, p_partial.pdp, how='left')\n",
    "                f_vals[tuple(subsetfeatures)] = self.center(p_joined.preds.values)\n",
    "        return f_vals\n",
    "\n",
    "    def compute_h_val(self,f_vals, selectedfeatures):\n",
    "        denom_els = f_vals[tuple(selectedfeatures)].copy()\n",
    "        numer_els = f_vals[tuple(selectedfeatures)].copy()\n",
    "        sign = -1.0\n",
    "        for n in range(len(selectedfeatures)-1, 0, -1):\n",
    "            for subfeatures in itertools.combinations(selectedfeatures, n):\n",
    "                print(tuple(subfeatures))\n",
    "                numer_els += sign * f_vals[tuple(subfeatures)]\n",
    "            sign *= -1.0\n",
    "        numer = np.sum(numer_els**2)\n",
    "        denom = np.sum(denom_els**2)\n",
    "        return math.sqrt(numer/denom) if numer < denom else np.nan\n",
    "\n",
    "    def compute_h_val_any(self,f_vals, allfeatures, selectedfeature):\n",
    "        otherfeatures = list(allfeatures)\n",
    "        otherfeatures.remove(selectedfeature)\n",
    "        denom_els = f_vals[tuple(allfeatures)].copy()\n",
    "        numer_els = denom_els.copy()\n",
    "        numer_els -= f_vals[(selectedfeature,)]\n",
    "        numer_els -= f_vals[tuple(otherfeatures)]\n",
    "        numer = np.sum(numer_els**2)\n",
    "        denom = np.sum(denom_els**2)\n",
    "        return math.sqrt(numer/denom) if numer < denom else np.nan\n",
    "\n",
    "    def compute_interactions(self,model,X_train,feature_all,feature_select_list):  \n",
    "        result_dict={}\n",
    "        total_time = 0  # Total time taken by the function\n",
    "        for i in range(len(feature_select_list)):\n",
    "            for j in range(len(feature_select_list)):\n",
    "                if i<j :\n",
    "                    print(i,j)\n",
    "                    try:\n",
    "                        current_features=[feature_select_list[i],feature_select_list[j]]\n",
    "                        start_time = time.time()  # Start time of the computation\n",
    "                        f_vals=self.compute_f_vals(model, X_train, feature_all,current_features)\n",
    "                        h_val=self.compute_h_val(f_vals,current_features)\n",
    "                        elapsed_time = time.time() - start_time  # Time taken for the computation\n",
    "                        if np.isnan(h_val):\n",
    "                            print('current h_val is nan, convert to 0')\n",
    "                            h_val = 0\n",
    "                        result_dict[tuple(current_features)]=h_val\n",
    "                        total_time += elapsed_time  # Add the elapsed time to the total time\n",
    "                        print(\"Elapsed time taken:\", elapsed_time)\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        result_dict[tuple(current_features)]=0\n",
    "                    print(result_dict[tuple(current_features)])\n",
    "        print(\"Total time taken:\", total_time)  # Print the total time taken\n",
    "        return result_dict\n",
    "\n",
    "    def h_index_weighted_matrix_compute(self,project_name):\n",
    "        \"\"\"\n",
    "        Compute the H-index weighted matrix for the given models and data.\n",
    "\n",
    "        Parameters:\n",
    "        - project_name (str): The name of the project, used for creating the directory where the images are saved.\n",
    "\n",
    "        This function computes the H-index weighted matrix for each model in the model committee. It first concatenates \n",
    "        the training and test datasets, and then computes the weighted matrix for each model based on its interactions and \n",
    "        scores. Finally, it computes the first and second order H matrices and saves them as heatmaps.\n",
    "\n",
    "        Returns:\n",
    "        - matrix_list (list): List of weighted matrices for each model.\n",
    "        - second_order_matrix (DataFrame): The second order H matrix.\n",
    "        - first_order_matrix (DataFrame): The first order H matrix.\n",
    "        - image_1st (str): Path to the saved image of the first order H matrix.\n",
    "        - image_2nd (str): Path to the saved image of the second order H matrix.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Define the directory to save the images\n",
    "        dir_path = os.path.join('.', project_name, 'h_index_weighted_matrix_compute')\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        \n",
    "        # Concatenate the training and test datasets\n",
    "        X_for_computing= pd.concat([self.current_data[0], self.current_data[1]])\n",
    "\n",
    "        def construct_matrix_weighted(target_dict,target_score):\n",
    "            # Construct a weighted matrix from the given dictionary and score\n",
    "            df=pd.DataFrame(columns=X_for_computing.columns,index=X_for_computing.columns)\n",
    "            for each in target_dict:\n",
    "                df.loc[each[0],each[1]]=target_dict[each]*target_score\n",
    "                df.loc[each[1],each[0]]=target_dict[each]*target_score\n",
    "            return df\n",
    "\n",
    "        # Initialize the total matrix list and total metric score\n",
    "        total_h_matrix_list=[]\n",
    "        total_metric_score=0\n",
    "\n",
    "        # For each model, compute the interactions and the weighted matrix, and add to the total matrix list and total metric score\n",
    "        for each_key in self.models.keys():\n",
    "            CURRENT_MODEL=self.models[each_key]\n",
    "            CURRENT_MODEL_DICT=self.compute_interactions(CURRENT_MODEL,X_for_computing,X_for_computing.columns,list(X_for_computing.columns))\n",
    "            if self.problem_type=='regression':\n",
    "                CURRENT_SCORE=self.models_scores[each_key][1][1]\n",
    "                CURRENT_DF=construct_matrix_weighted(CURRENT_MODEL_DICT,CURRENT_SCORE)\n",
    "            else:\n",
    "                CURRENT_SCORE=self.models_scores[each_key][1][4]\n",
    "                CURRENT_DF=construct_matrix_weighted(CURRENT_MODEL_DICT,CURRENT_SCORE)\n",
    "            total_h_matrix_list.append(CURRENT_DF)\n",
    "            total_metric_score+=CURRENT_SCORE\n",
    "        \n",
    "        # Compute the weighted matrix\n",
    "        Weighted_Matrix = sum([df.values for df in total_h_matrix_list]) / total_metric_score\n",
    "        Weighted_Matrix = pd.DataFrame(Weighted_Matrix, index=total_h_matrix_list[0].index, columns=total_h_matrix_list[0].columns)\n",
    "        Weighted_Matrix = Weighted_Matrix.fillna(0)\n",
    "        Weighted_Matrix = Weighted_Matrix / Weighted_Matrix.max().max()\n",
    "        divided_matrices = [df / total_metric_score for df in total_h_matrix_list]\n",
    "        \n",
    "        def Second_Order_H_Matrix_Plot(H_DF_WEIGHTED):\n",
    "            # Create and save the heatmap for the second order H matrix\n",
    "            # Set the figure size\n",
    "            heatmap_figsize = (max(12, H_DF_WEIGHTED.shape[0] * 0.6), max(12, H_DF_WEIGHTED.shape[0] * 0.6))\n",
    "            plt.figure(figsize=heatmap_figsize)\n",
    "            # Create the heatmap\n",
    "            sns.heatmap(H_DF_WEIGHTED, annot=False, cmap=\"hot_r\", cbar_kws={'label': 'Second Order H Statistics'})\n",
    "            # Set the title and labels\n",
    "            plt.title('Weighted Second Order H statistics')\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.xlabel('Features')\n",
    "            plt.ylabel('Features')\n",
    "            plt.tight_layout()\n",
    "            # Save the figure\n",
    "            image_file = os.path.join(dir_path, \"Second_Order_H_Matrix_Plot.png\")\n",
    "            plt.savefig(image_file)\n",
    "            plt.show()\n",
    "            return image_file\n",
    "\n",
    "        def First_Order_H_Matrix_Plot(Model_Names_List, Matrix_list):\n",
    "            # Create and save the heatmap for the first order H matrix\n",
    "            # Compute the summary matrix\n",
    "            summary_matrix = pd.DataFrame(index=Model_Names_List)\n",
    "            for name, matrix in zip(Model_Names_List, Matrix_list):\n",
    "                row_values = matrix.sum(axis=1).T  # Sum over rows\n",
    "                for model_name, value in row_values.iteritems():\n",
    "                    summary_matrix.loc[name, model_name] = value\n",
    "            # Normalize the summary matrix\n",
    "            summary_matrix = summary_matrix.fillna(0)\n",
    "            summary_matrix = summary_matrix / summary_matrix.max().max()\n",
    "            # Set the figure size\n",
    "            heatmap_figsize = (max(12, summary_matrix.shape[0] * 0.6), max(12, summary_matrix.shape[0] * 0.6))\n",
    "            plt.figure(figsize=heatmap_figsize)\n",
    "            # Create the heatmap\n",
    "            sns.heatmap(summary_matrix, annot=False, cmap=\"hot_r\", cbar_kws={'label': 'First Order H Statistics'})\n",
    "            # Set the title and labels\n",
    "            plt.title('First Order H Statistics')\n",
    "            plt.xticks(rotation=45, ha=\"right\")\n",
    "            plt.xlabel('Models')\n",
    "            plt.ylabel('Models')\n",
    "            plt.tight_layout()\n",
    "            # Save the figure\n",
    "            image_file = os.path.join(dir_path, \"First_Order_H_Matrix_Plot.png\")\n",
    "            plt.savefig(image_file)\n",
    "            plt.show()\n",
    "            return summary_matrix,image_file\n",
    "        \n",
    "        # Compute the first and second order H matrices and save as heatmaps\n",
    "        first_order_matrix,image_1st=First_Order_H_Matrix_Plot(list(self.models.keys()), divided_matrices)\n",
    "        second_order_matrix=Weighted_Matrix\n",
    "        image_2nd=Second_Order_H_Matrix_Plot(Weighted_Matrix)\n",
    "        matrix_list=divided_matrices\n",
    "        self.second_order_matrix=second_order_matrix\n",
    "        plt.close()\n",
    "        return matrix_list,second_order_matrix,first_order_matrix,image_1st,image_2nd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac1b4aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import pandas as pd\n",
    "import keras\n",
    "from sklearn.preprocessing import *\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "ANN_loaded_l=keras.models.load_model(\"./All_HER/ANN_model_l.h5\")\n",
    "ANN_loaded_m=keras.models.load_model(\"./All_HER/ANN_model_m.h5\")\n",
    "ANN_loaded_h=keras.models.load_model(\"./All_HER/ANN_model_h.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7010b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_model_compute(ANN_loaded,project_name)\n",
    "    CMT=ML_algorithm_committee()\n",
    "    CMT.models={'ann':ANN_loaded}\n",
    "    CMT.problem_type='regression'\n",
    "    seed=1234\n",
    "    Minmaxsc  = MinMaxScaler(feature_range=(0, 1))\n",
    "    Minmaxsc2  = MinMaxScaler(feature_range=(0, 1))\n",
    "    Stdsc  = StandardScaler()\n",
    "    Stdsc2  = StandardScaler()\n",
    "    MAsc  = MaxAbsScaler()\n",
    "    MAsc2  = MaxAbsScaler()\n",
    "    Rsc  = RobustScaler()\n",
    "    Rsc2  = RobustScaler()\n",
    "    database=pd.read_csv('processed_database.csv')\n",
    "    data_output_full=database.iloc[:,1]\n",
    "    data_input_full=database.iloc[:,2:]\n",
    "    data_input_full_ANN=Stdsc.fit_transform(data_input_full)\n",
    "    data_output_full_ANN=Stdsc2.fit_transform(np.array(data_output_full).reshape(-1,1))\n",
    "    X_train_ANN,X_test_ANN,y_train_ANN,y_test_ANN=train_test_split(data_input_full_ANN,data_output_full_ANN,test_size=0.05,random_state=seed)\n",
    "    # Getting column names\n",
    "    input_column_names = data_input_full.columns\n",
    "    output_column_name = data_output_full.name # Assuming data_output_full is a Series\n",
    "\n",
    "    # Converting NumPy arrays back to DataFrame\n",
    "    X_train_ANN_df = pd.DataFrame(X_train_ANN, columns=input_column_names)\n",
    "    X_test_ANN_df = pd.DataFrame(X_test_ANN, columns=input_column_names)\n",
    "\n",
    "    # If your y_train_ANN and y_test_ANN are 1-D arrays, you can convert them into a Series:\n",
    "    y_train_ANN_series = pd.Series(y_train_ANN.flatten(), name=output_column_name)\n",
    "    y_test_ANN_series = pd.Series(y_test_ANN.flatten(), name=output_column_name)\n",
    "    CMT.current_data=(X_train_ANN_df,X_test_ANN_df,y_train_ANN_series,y_test_ANN_series)\n",
    "    CMT.models_scores={'ann': ([None,1,None,None],[None,1,None,None])}\n",
    "    ###H interaction compute###\n",
    "    matrix_list,second_order_matrix,first_order_matrix,h_mat_image_1st,h_mat_image_2nd=CMT.h_index_weighted_matrix_compute(project_name=project_name)\n",
    "    ###create H interaction directory if it doesn't exist###\n",
    "    save_dir_H = f\"./{project_name}/H_Compute_Save\"\n",
    "    os.makedirs(save_dir_H, exist_ok=True)\n",
    "    ###Save the data\n",
    "    np.save(os.path.join(save_dir_H, \"matrix_list.npy\"), matrix_list)\n",
    "    np.save(os.path.join(save_dir_H, \"second_order_matrix.npy\"), second_order_matrix)\n",
    "    np.save(os.path.join(save_dir_H, \"first_order_matrix.npy\"), first_order_matrix)\n",
    "    return matrix_list,second_order_matrix,first_order_matrix,h_mat_image_1st,h_mat_image_2nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97483f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_list_l,second_order_matrix_l,first_order_matrix_l,h_mat_image_1st_l,h_mat_image_2nd_l=single_model_compute(ANN_loaded_l,'All_HER_l')\n",
    "matrix_list_m,second_order_matrix_m,first_order_matrix_m,h_mat_image_1st_m,h_mat_image_2nd_m=single_model_compute(ANN_loaded_m,'All_HER_m')\n",
    "matrix_list_h,second_order_matrix_h,first_order_matrix_h,h_mat_image_1st_h,h_mat_image_2nd_h=single_model_compute(ANN_loaded_h,'All_HER_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae8a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Full features\n",
    "# fig, ax = plt.subplots(figsize=(5,30))\n",
    "# features = first_order_matrix.loc['ann'].sort_values(ascending=False)\n",
    "# colors = sns.color_palette(\"Blues\", len(features))[::-1]  # Reverse the color palette\n",
    "# sns.barplot(y=features.index, x=features.values, palette=colors, ax=ax)\n",
    "# plt.title('Full features')\n",
    "# plt.xlabel('Feature Importance')\n",
    "# plt.ylabel('Features')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dede07d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed391e7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
